<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering">
    <meta name="author"
          content="Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, Fredo Durand">

    <title>Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Light Field Networks:<br>Neural Scene Representations with Single-Evaluation Rendering</h2>
    <h3>NeurIPS 2021 (Spotlight)</h3>
    <hr>
    <p class="authors">
        <a href="http://www.stanford.edu/~sitzmann/"> Vincent Sitzmann*</a>,
        <a href="https://math.columbia.edu/~skr/"> Semon Rezchikov*</a>,
        <a href="https://billf.mit.edu/"> William T. Freeman</a>,</br>
        <a href="https://mitibmwatsonailab.mit.edu/people/joshua-tenenbaum/"> Joshua B. Tenenbaum</a>,
        <a href="https://people.csail.mit.edu/fredo/"> Fredo Durand</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="http://arxiv.org/abs/2106.02634">Paper</a>
        <!--        <a class="btn btn-primary disabled" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>-->
        <!--        <a class="btn btn-primary disabled" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>-->
        <a class="btn btn-primary" href="https://github.com/vsitzmann/light-field-networks">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/folders/15u6WD0zSBXzu8jZBF-Sn5n01F2HSxFCp">Data</a>
    </div>
</div>


<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/x3sSreTNFw4" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics,
            computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a
            promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation,
            Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a
            360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray
            from an LFN requires only a *single* network evaluation, as opposed to hundreds of evaluations per ray for
            ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of
            simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light
            field reconstruction from as little as a single image observation. This results in dramatic reductions in
            time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field
            via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the
            analytical differentiability of neural implicit representations and a novel parameterization of light space,
            we further demonstrate the extraction of sparse depth maps from LFNs.
        </p>
    </div>

    <div class="section">
        <h2>The trouble with SRNs, NeRF & Co.</h2>
        <hr>
        <p>
            3D-structured Neural Scene Representations have recently enjoyed great success, and have enabled new applications
            in graphics, vision, and artificial intelligence.
            However, they are currently severely limited by the neural rendering algorithms they require for training and
            testing: Both volumetric rendering as in NeRF and sphere-tracing as in Scene Representation Networks
            require <b>hundreds</b> of evaluations of the representation per ray, resulting in forward pass times on the order of
            <b>tens of seconds</b> for a single, 256x256 image. This cost is incurred both at training and testing time,
            and training requires backpropagating through this extremely expensive rendering algorithm.
            This makes these neural scene representations borderline infeasible for use in all but computer graphics tasks, and even here,
            additional tricks - such as caching and hybrid explicit-implicit representations - are required to achieve real-time
            framerates.
        </p>
    </div>

    <div class="section">
        <h2>Light Field Networks</h2>
        <hr>
        <p>
            We introduce Light Field Networks, or short LFNs. Instead of mapping a 3D world coordinate to whatever is at that
            coordinate, LFNs directly map an <b>oriented ray</b> to whatever is <b>observed</b> by that oriented ray.
            In this manner, LFNs parameterize the <b>full 360-degree light field</b> of the underlying 3D scene.
            This means that LFNs only require a <b>single</b> evaluation of the neural implicit representation per ray.
            This unlocks rendering at framerates of >500 FPS, and with a minimal memory footprint. Below,
            we compare rendering an LFN to the recently proposed PixelNeRF - LFNs accelerate rendering by a factor of about 15,000.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/lfn_vs_pixelnerf_compressed.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>The geometry of light fields</h2>
        <hr>
        <p>
            Unintuitevly, LFNs do <b>not</b> only encode the appearance of the underlying 3D scene, <b>but also its geometry</b>.
            Our novel parameterization of light fields via the mathematically convenient plucker
            coordinates, together with the unique properties of Neural Implicit Representations, allows us to <b>extract sparse depth maps</b>
            of the underlying 3D scene in <b>constant time, without ray-marching!</b> We achieve this by deriving a
            relationship of an LFN's derivatives to the scene's geometry: at a high level, the geometry of the levelsets of the
            4D light field encode the geometry of the underlying scene, and these levelsets can be efficiently accessed via automatic
            differentiation. This is in contrast to 3D-structured
            representations, which require ray-marching to extract any representation of the scene's geometry.
            Below, we show sparse depth maps extracted from LFNs that were trained to represent simple room-scale environments.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/rooms_360_compressed.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </div>


    <div class="section">
        <h2>Meta-learning LFNs</h2>
        <hr>
        <p>
            While 3D-structured neural representations ensure multi-view consistency via ray-marching, Light Field Networks
            are not naturally multi-view consistent. To overcome this challenge, we leverage meta-learning via hypernetworks
            to learn a space of multi-view consistent light fields.
            As a corollary, we can leverage this learned prior to reconstruct an LFN from <b>only a single image observation</b>!
            In this regime, LFNs outperform existing globally conditioned neural scene representations such as Scene Representation Networks
            or the Differentiable Volumetric Renderer, while rendering in real-time and requiring orders of magnitude less memory.
        </p>
    </div>

        <div class="section">
            <h2>Related Projects</h2>
            <hr>
            <p>
                Check out our related projects on neural scene representations! <br>
            </p>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/poisson_convergence_15s_label.mp4" type="video/mp4">
                    </video>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="http://vsitzmann.github.io/siren/">Implicit Neural Representations with Periodic Activation Functions</a>
                    </div>
                    <div>
                        We propose a new neural network architecture for implicit neural representations that can accurately
                        fit complex signals, such as room-scale SDFs, video, and audio, and allows us to supervise implicit
                        representations via their gradients to solve boundary value problems!
                    </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/metasdf_steps_comp.mp4" type="video/mp4">
                    </video>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="http://vsitzmann.github.io/metasdf/">MetaSDF: Meta-learning Signed Distance
                            Functions</a>
                    </div>
                    <div>
                        We identify a key relationship between generalization across implicit neural representations and
                        meta-
                        learning, and propose to leverage gradient-based meta-learning for learning priors over deep
                        signed distance
                        functions. This allows us to reconstruct SDFs an order of magnitude faster than the auto-decoder
                        framework,
                        with no loss in performance!
                    </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/SRNs.gif' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="http://vsitzmann.github.io/srns/">Scene Representation Networks: Continuous
                            3D-Structure-Aware Neural Scene Representations</a>

                    </div>
                    <div>
                        A continuous, 3D-structure-aware neural scene representation that encodes both geometry and
                        appearance,
                        supervised only in 2D via a neural renderer, and generalizes for 3D reconstruction from a single
                        posed 2D image.
                    </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/srn_seg_repimage.jpg' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://www.computationalimaging.org/publications/semantic-srn/">Inferring Semantic
                            Information with 3D Neural Scene Representations
                        </a>
                    </div>
                    <div>
                        We demonstrate that the features learned by neural implicit scene representations are useful for
                        downstream
                        tasks, such as semantic segmentation, and propose a model that can learn to perform continuous
                        3D
                        semantic segmentation on a class of objects (such as chairs) given only a single, 2D (!)
                        semantic label map!
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>Paper</h2>
                <hr>
                <div>
                    <div class="list-group">
                        <a href="http://arxiv.org/abs/2106.02634"
                           class="list-group-item">
                            <img src="img/paper_thumbnail.png"
                                 style="width:100%; margin-right:-20px; margin-top:-10px;">
                        </a>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>Bibtex</h2>
                <hr>
                <div class="bibtexsection">
                    @inproceedings{sitzmann2021lfns,
                        author = {Sitzmann, Vincent
                                  and Rezchikov, Semon
                                  and Freeman, William T.
                                  and Tenenbaum, Joshua B.
                                  and Durand, Fredo},
                        title = {Light Field Networks: Neural Scene Representations
                                 with Single-Evaluation Rendering},
                        booktitle = {Proc. NeurIPS},
                        year={2021}
                    }
                </div>
            </div>

            <hr>

            <footer>
                <p>Send feedback and questions to <a href="vsitzmann.github.io">Vincent Sitzmann</a></p>
            </footer>
        </div>


        <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
                integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
                crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
                integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
                crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
                integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
                crossorigin="anonymous"></script>

</body>
</html>
